import torch
from torch import nn

import math
from pathlib import Path

from .nn_base import NNBase
from ...preprocessing import VALUE_COLS, VEGETATION_LABELS


class Recurrent(NNBase):
    """A simple feedforward neural network
    """

    def __init__(self, data=Path('data'), arrays=Path('data/processed/arrays'),
                 hide_vegetation=False):

        features_per_month = len(VALUE_COLS)
        if hide_vegetation:
            features_per_month -= len(VEGETATION_LABELS)

        super().__init__(RNN(features_per_month, [256]),
                         data, arrays, hide_vegetation)


class RNN(nn.Module):
    """
    A crop yield conv net.
    For a description of the parameters, see the RNNModel class.
    """
    def __init__(self, features_per_month, dense_features, hidden_size=128,
                 rnn_dropout=0.25):
        super().__init__()

        dense_features.insert(0, hidden_size)
        if dense_features[-1] != 1:
            dense_features.append(1)

        self.dropout = nn.Dropout(rnn_dropout)
        self.rnn = UnrolledRNN(input_size=features_per_month,
                               hidden_size=hidden_size,
                               batch_first=True)
        self.hidden_size = hidden_size

        self.dense_layers = nn.ModuleList([
            nn.Linear(in_features=dense_features[i-1],
                      out_features=dense_features[i])
            for i in range(1, len(dense_features))
        ])

        self.initialize_weights()

    def initialize_weights(self):

        sqrt_k = math.sqrt(1 / self.hidden_size)
        for parameters in self.rnn.parameters():
            for pam in parameters:
                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)

        for dense_layer in self.dense_layers:
            nn.init.kaiming_uniform_(dense_layer.weight.data)
            nn.init.constant_(dense_layer.bias.data, 0)

    def forward(self, x):
        """
        If return_last_dense is true, the feature vector generated by the second to last
        dense layer will also be returned. This is then used to train a Gaussian Process model.
        """

        sequence_length = x.shape[1]

        hidden_state = torch.zeros(1, x.shape[0], self.hidden_size)
        cell_state = torch.zeros(1, x.shape[0], self.hidden_size)

        if x.is_cuda:
            hidden_state = hidden_state.cuda()
            cell_state = cell_state.cuda()

        for i in range(sequence_length):
            # The reason the RNN is unrolled here is to apply dropout to each timestep;
            # The rnn_dropout argument only applies it after each layer.
            # https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper
            input_x = x[:, i, :].unsqueeze(1)
            _, (hidden_state, cell_state) = self.rnn(input_x,
                                                     (hidden_state, cell_state))
            hidden_state = self.dropout(hidden_state)

        x = hidden_state.squeeze(0)
        for layer_number, dense_layer in enumerate(self.dense_layers):
            x = dense_layer(x)
        return x


class UnrolledRNN(nn.Module):
    """An unrolled RNN. The motivation for this is mainly so that we can explain this model using
    the shap deep explainer, but also because we unroll the RNN anyway to apply dropout.
    """

    def __init__(self, input_size, hidden_size, batch_first=True):
        super().__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.batch_first = batch_first

        self.forget_gate = nn.Sequential(*[
            nn.Linear(in_features=input_size + hidden_size, out_features=hidden_size,
                      bias=True), nn.Sigmoid()])

        self.update_gate = nn.Sequential(*[
            nn.Linear(in_features=input_size + hidden_size, out_features=hidden_size,
                      bias=True), nn.Sigmoid()
        ])

        self.update_candidates = nn.Sequential(*[
            nn.Linear(in_features=input_size + hidden_size, out_features=hidden_size,
                      bias=True), nn.Tanh()
        ])

        self.output_gate = nn.Sequential(*[
            nn.Linear(in_features=input_size + hidden_size, out_features=hidden_size,
                      bias=True), nn.Sigmoid()
        ])

        self.cell_state_activation = nn.Tanh()

    def forward(self, x, state):
        hidden, cell = state

        if self.batch_first:
            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)

        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))
        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))
        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))

        updated_cell = (forget_state * cell) + (update_state * cell_candidates)

        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))
        updated_hidden = output_state * self.cell_state_activation(updated_cell)

        if self.batch_first:
            updated_hidden = torch.transpose(updated_hidden, 0, 1)
            updated_cell = torch.transpose(updated_cell, 0, 1)

        return updated_hidden, (updated_hidden, updated_cell)

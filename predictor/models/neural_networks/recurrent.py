import torch
from torch import nn

import math
from pathlib import Path

from .nn_base import NNBase
from ...preprocessing import VALUE_COLS, VEGETATION_LABELS


class Recurrent(NNBase):
    """A simple feedforward neural network
    """

    def __init__(self, arrays=Path('data/processed/arrays'), hide_vegetation=False):

        features_per_month = len(VALUE_COLS)
        if hide_vegetation:
            features_per_month -= len(VEGETATION_LABELS)

        super().__init__(RNN(features_per_month, [256]),
                         arrays, hide_vegetation)


class RNN(nn.Module):
    """
    A crop yield conv net.
    For a description of the parameters, see the RNNModel class.
    """
    def __init__(self, features_per_month, dense_features, hidden_size=128,
                 rnn_dropout=0.25):
        super().__init__()

        dense_features.insert(0, hidden_size)
        if dense_features[-1] != 1:
            dense_features.append(1)

        self.dropout = nn.Dropout(rnn_dropout)
        self.rnn = nn.LSTM(input_size=features_per_month,
                           hidden_size=hidden_size,
                           num_layers=1,
                           batch_first=True)
        self.hidden_size = hidden_size

        self.dense_layers = nn.ModuleList([
            nn.Linear(in_features=dense_features[i-1],
                      out_features=dense_features[i])
            for i in range(1, len(dense_features))
        ])

        self.initialize_weights()

    def initialize_weights(self):

        sqrt_k = math.sqrt(1 / self.hidden_size)
        for parameters in self.rnn.all_weights:
            for pam in parameters:
                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)

        for dense_layer in self.dense_layers:
            nn.init.kaiming_uniform_(dense_layer.weight.data)
            nn.init.constant_(dense_layer.bias.data, 0)

    def forward(self, x):
        """
        If return_last_dense is true, the feature vector generated by the second to last
        dense layer will also be returned. This is then used to train a Gaussian Process model.
        """

        sequence_length = x.shape[1]

        hidden_state = torch.zeros(1, x.shape[0], self.hidden_size)
        cell_state = torch.zeros(1, x.shape[0], self.hidden_size)

        if x.is_cuda:
            hidden_state = hidden_state.cuda()
            cell_state = cell_state.cuda()

        for i in range(sequence_length):
            # The reason the RNN is unrolled here is to apply dropout to each timestep;
            # The rnn_dropout argument only applies it after each layer.
            # https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper
            input_x = x[:, i, :].unsqueeze(1)
            _, (hidden_state, cell_state) = self.rnn(input_x,
                                                     (hidden_state, cell_state))
            hidden_state = self.dropout(hidden_state)

        x = hidden_state.squeeze(0)
        for layer_number, dense_layer in enumerate(self.dense_layers):
            x = dense_layer(x)
        return x
